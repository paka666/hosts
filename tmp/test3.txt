#!/usr/bin/env python3
import os
import re
import zipfile
import tempfile
import requests
import ipaddress
import json
import gzip
import unicodedata
import logging
import chardet
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from ipaddress import IPv4Address, IPv6Address, IPv4Network, IPv6Network
from time import sleep
from typing import List, Tuple, Set
from urllib.parse import urlparse

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)

# 组和 URL 映射 (保持不变)
GROUPS = {
    # ... 保持原有的GROUPS配置不变
}

def detect_encoding(file_path: Path) -> str:
    """检测文件编码"""
    try:
        with open(file_path, 'rb') as f:
            raw_data = f.read()
            result = chardet.detect(raw_data)
            encoding = result['encoding'] or 'utf-8'
            if encoding.lower() in ['utf-8', 'ascii', 'latin-1', 'iso-8859-1', 'windows-1252']:
                return encoding
            else:
                return 'utf-8'
    except Exception as e:
        logging.warning(f"Encoding detection failed for {file_path}: {e}")
        return 'utf-8'

def enhanced_universal_clean(line: str) -> str:
    """增强的清理函数，彻底处理各种字符和格式问题"""
    if not isinstance(line, str):
        return ""
    
    # 深度Unicode标准化
    line = unicodedata.normalize('NFKD', line)
    
    # 全角转半角
    line = ''.join(
        chr(ord(c) - 65248) if 65281 <= ord(c) <= 65374 else c 
        for c in line
    )
    
    # 移除BOM和其他控制字符（保留\t, \n, \r）
    line = re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F]', '', line)
    
    # 移除行内注释（#!;）并保留第一个部分
    line = re.split(r'[#!;]', line, 1)[0]
    
    # 彻底去除首尾空白字符
    line = line.strip()
    
    # 标准化行内空白（多个空白字符替换为单个空格）
    line = re.sub(r'\s+', ' ', line)
    
    # 再次去除首尾空白
    line = line.strip()
    
    # 检查是否为空或注释行
    if not line or line.startswith(('#', '!', ';')):
        return ""
    
    return line

def clean_lines(lines: List[str]) -> List[str]:
    """清理多行文本"""
    if not lines:
        return []
    
    cleaned_set: Set[str] = set()
    for line in lines:
        cleaned = enhanced_universal_clean(line)
        if cleaned:
            cleaned_set.add(cleaned)
    
    return sorted(cleaned_set)

def download_file(url: str, path: Path, retries: int = 3) -> bool:
    """下载文件，支持重试"""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    
    for attempt in range(1, retries + 1):
        try:
            response = requests.get(
                url, 
                stream=True, 
                timeout=60, 
                headers=headers,
                allow_redirects=True
            )
            response.raise_for_status()
            
            with open(path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
            
            if path.stat().st_size > 0:
                logging.info(f"Successfully downloaded {url} to {path}")
                return True
            else:
                logging.warning(f"Downloaded empty file from {url}")
                continue
                
        except requests.exceptions.RequestException as e:
            logging.warning(f"Download failed for {url} (attempt {attempt}/{retries}): {e}")
            if attempt < retries:
                sleep_time = 2 ** attempt
                sleep(sleep_time)
    
    logging.error(f"Failed to download {url} after {retries} attempts")
    return False

def extract_gz(gz_path: Path, retries: int = 2) -> List[str]:
    """解压.gz文件，支持重试"""
    for attempt in range(retries):
        try:
            with gzip.open(gz_path, 'rt', encoding='utf-8', errors='ignore') as f:
                return f.readlines()
        except Exception as e:
            logging.warning(f"GZ extraction failed for {gz_path} (attempt {attempt + 1}/{retries}): {e}")
            if attempt < retries - 1:
                sleep(1)
    
    return []

def extract_zip(zip_path: Path, extract_to: Path, retries: int = 2) -> bool:
    """解压.zip文件，支持重试"""
    for attempt in range(retries):
        try:
            with zipfile.ZipFile(zip_path, 'r') as z:
                z.extractall(extract_to)
            logging.info(f"Successfully extracted {zip_path}")
            return True
        except Exception as e:
            logging.warning(f"ZIP extraction failed for {zip_path} (attempt {attempt + 1}/{retries}): {e}")
            if attempt < retries - 1:
                sleep(1)
    
    return False

def diff_rules(a_file: str, b_file: str, output_file: str = 'adh/ip-blocklist.txt') -> int:
    """计算 a - b：从 blocklist.txt 减去 domain-blocklist.txt，输出 IP 规则，去除 || 和 ^"""
    b_rules = set()
    a_path = Path(a_file)
    b_path = Path(b_file)
    
    if not a_path.exists():
        logging.error(f"Input file {a_file} does not exist")
        return 0
    if not b_path.exists():
        logging.error(f"Input file {b_file} does not exist")
        return 0
    
    # 加载 domain-blocklist.txt 到 set
    enc_b = detect_encoding(b_path)
    try:
        with open(b_path, 'r', encoding=enc_b, errors='ignore') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith(('#', '!')):
                    b_rules.add(line)
    except Exception as e:
        logging.error(f"Failed to read {b_file}: {e}")
        return 0
    
    # 遍历 blocklist.txt，输出不在 domain-blocklist.txt 的行
    ip_count = 0
    enc_a = detect_encoding(a_path)
    try:
        with open(a_path, 'r', encoding=enc_a, errors='ignore') as a_f, \
             open(output_file, 'w', encoding='utf-8') as out_f:
            for line in a_f:
                line = line.strip()
                if line and not line.startswith(('#', '!')) and line not in b_rules:
                    cleaned_line = line
                    if cleaned_line.startswith('||'):
                        cleaned_line = cleaned_line[2:]
                    if cleaned_line.endswith('^'):
                        cleaned_line = cleaned_line[:-1]
                    if cleaned_line:
                        out_f.write(cleaned_line + '\n')
                        ip_count += 1
    except Exception as e:
        logging.error(f"Failed to process diff_rules: {e}")
        return 0
    
    logging.info(f"Generated {output_file} with {ip_count} IP rules")
    return ip_count

def process_group(urls: List[str], group_name: str, temp_dir: Path) -> List[str]:
    """处理标准文本组 - 改进版本，统一参数顺序"""
    all_lines = []
    
    def download_and_process(url_idx):
        url, idx = url_idx
        filename = f"{group_name}_{idx}.txt"  # 统一使用.txt后缀
        filepath = temp_dir / filename
        if download_file(url, filepath):
            enc = detect_encoding(filepath)
            try:
                with open(filepath, 'r', encoding=enc, errors='ignore') as f:
                    return f.readlines()
            except Exception as e:
                logging.warning(f"Failed to read {filepath}: {e}")
        return []
    
    with ThreadPoolExecutor(max_workers=8) as executor:
        futures = [executor.submit(download_and_process, (url, i)) for i, url in enumerate(urls)]
        for future in as_completed(futures):
            lines = future.result()
            if lines:
                all_lines.extend(lines)
    
    return clean_lines(all_lines)

def process_c(urls: List[str], temp_dir: Path) -> List[str]:
    """处理c组（管道分隔格式）"""
    ips = []
    with ThreadPoolExecutor(max_workers=4) as ex:
        future_to_path = {
            ex.submit(download_file, url, temp_dir / f"c_{i}.txt"): temp_dir / f"c_{i}.txt" 
            for i, url in enumerate(urls)
        }
        for future in as_completed(future_to_path):
            path = future_to_path[future]
            if future.result():
                enc = detect_encoding(path)
                try:
                    with open(path, 'r', encoding=enc, errors='ignore') as f:
                        for line in f:
                            parts = [p.strip() for p in line.split('|')]
                            if len(parts) >= 3 and parts[2]:
                                ips.append(parts[2])
                except IOError as e:
                    logging.warning(f"Failed to read {path}: {e}")
    
    return clean_lines(ips)

def range_to_cidrs(range_str: str) -> List[str]:
    """将IP范围转换为CIDR"""
    if '-' not in range_str:
        return [range_str.strip()]
    
    try:
        start_str, end_str = range_str.split('-')
        start_ip = IPv4Address(start_str.strip())
        end_ip = IPv4Address(end_str.strip())
        return [str(net) for net in ipaddress.summarize_address_range(start_ip, end_ip)]
    except (ValueError, TypeError):
        return []

def process_d(group: str, urls: List[str], is_p2p: bool, temp_dir: Path) -> List[str]:
    """处理d组（gzip压缩格式）"""
    lines = []
    with ThreadPoolExecutor(max_workers=4) as ex:
        future_to_path = {
            ex.submit(download_file, url, temp_dir / f"{group}_{i}.gz"): temp_dir / f"{group}_{i}.gz" 
            for i, url in enumerate(urls)
        }
        for future in as_completed(future_to_path):
            gz_path = future_to_path[future]
            if future.result():
                gz_lines = extract_gz(gz_path)
                if is_p2p:
                    for line in gz_lines:
                        if ':' in line:
                            range_part = line.split(':', 1)[1].strip()
                            cidrs = range_to_cidrs(range_part)
                            lines.extend(cidrs)
                else:
                    lines.extend(gz_lines)
    
    return clean_lines(lines)

def process_e(urls: List[str], temp_dir: Path) -> List[str]:
    """处理e组（JSON格式）"""
    cidrs = []
    with ThreadPoolExecutor(max_workers=2) as ex:
        future_to_path = {
            ex.submit(download_file, url, temp_dir / f"e_{i}.json"): temp_dir / f"e_{i}.json" 
            for i, url in enumerate(urls)
        }
        for future in as_completed(future_to_path):
            path = future_to_path[future]
            if future.result():
                enc = detect_encoding(path)
                try:
                    with open(path, 'r', encoding=enc) as f:
                        for line in f:
                            try:
                                data = json.loads(line)
                                if 'cidr' in data and data.get('type') != 'metadata':
                                    cidrs.append(data['cidr'])
                            except json.JSONDecodeError:
                                continue
                except IOError as e:
                    logging.warning(f"Failed to read {path}: {e}")
    
    return clean_lines(cidrs)

def process_f(group: str, urls: List[str], regex: str, temp_dir: Path) -> List[str]:
    """处理f组（防火墙规则格式）"""
    items = []
    with ThreadPoolExecutor(max_workers=4) as ex:
        future_to_path = {
            ex.submit(download_file, url, temp_dir / f"{group}_{i}.rules"): temp_dir / f"{group}_{i}.rules" 
            for i, url in enumerate(urls)
        }
        for future in as_completed(future_to_path):
            path = future_to_path[future]
            if future.result():
                enc = detect_encoding(path)
                try:
                    with open(path, 'r', encoding=enc, errors='ignore') as f:
                        content = f.read()
                        matches = re.findall(regex, content)
                        if group == 'f3':
                            for m in matches:
                                items.extend([
                                    item.strip() for item in m.split(',') 
                                    if item.strip() and not item.strip().startswith('#')
                                ])
                        elif group == 'f4':
                            for ip, mask in matches:
                                try:
                                    prefix = bin(int(IPv4Address(mask))).count('1')
                                    items.append(f"{ip}/{prefix}")
                                except ValueError:
                                    continue
                        else:
                            items.extend(matches)
                except IOError as e:
                    logging.warning(f"Failed to read {path}: {e}")
    
    return clean_lines(items)

def extract_maltrail_ips_domains(lines: List[str]) -> Tuple[List[str], List[str]]:
    """
    专门处理maltrail格式的复杂规则
    整合了a.py和b.py的最佳实践，并参考2.md的完善建议
    """
    # IPv4模式（精确匹配）
    ipv4_pattern = r'^((?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?))(?:/(?:0|[1-9]|[12][0-9]|3[0-2])(?!\d))?$'

    # IPv6模式（精确匹配）
    ipv6_pattern = r'^(?:(([0-9A-Fa-f]{1,4}:){7}([0-9A-Fa-f]{1,4}|:))|(([0-9A-Fa-f]{1,4}:){6}(:[0-9A-Fa-f]{1,4}|((25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)(\.(25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)){3})|:))|(([0-9A-Fa-f]{1,4}:){5}(((:[0-9A-Fa-f]{1,4}){1,2})|:((25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)(\.(25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)){3})|:))|(([0-9A-Fa-f]{1,4}:){4}(((:[0-9A-Fa-f]{1,4}){1,3})|((:[0-9A-Fa-f]{1,4})?:((25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)(\.(25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)){3}))|:))|(([0-9A-Fa-f]{1,4}:){3}(((:[0-9A-Fa-f]{1,4}){1,4})|((:[0-9A-Fa-f]{1,4}){0,2}:((25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)(\.(25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)){3}))|:))|(([0-9A-Fa-f]{1,4}:){2}(((:[0-9A-Fa-f]{1,4}){1,5})|((:[0-9A-Fa-f]{1,4}){0,3}:((25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)(\.(25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)){3}))|:))|(([0-9A-Fa-f]{1,4}:){1}(((:[0-9A-Fa-f]{1,4}){1,6})|((:[0-9A-Fa-f]{1,4}){0,4}:((25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)(\.(25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)){3}))|:))|(:(((:[0-9A-Fa-f]{1,4}){1,7})|((:[0-9A-Fa-f]{1,4}){0,5}:((25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)(\.(25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)){3}))|:)))(/(?:0|[1-9]|[1-9][0-9]|1[0-1][0-9]|12[0-8])(?!\d))?$'

    ip_list = []
    domain_list = []

    for line in lines:
        original_line = line
        added = False

        # 1. 检查纯IPv4/IPv4 CIDR
        match = re.match(ipv4_pattern, line)
        if match:
            candidate = match.group(1)
            try:
                if '/' in candidate:
                    net = IPv4Network(candidate, strict=True)
                    if str(net) == candidate:
                        ip_list.append(candidate)
                        added = True
                else:
                    addr = IPv4Address(candidate)
                    if str(addr) == candidate:
                        ip_list.append(candidate)
                        added = True
            except (ValueError, AddressValueError, NetmaskValueError):
                pass

        if added:
            continue

        # 2. 检查纯IPv6/IPv6 CIDR
        match = re.match(ipv6_pattern, line, re.IGNORECASE)
        if match:
            candidate = match.group(0)
            try:
                if '/' in candidate:
                    net = IPv6Network(candidate, strict=True)
                    if str(net) == candidate:
                        ip_list.append(candidate)
                        added = True
                else:
                    addr = IPv6Address(candidate)
                    if str(addr) == candidate:
                        ip_list.append(candidate)
                        added = True
            except (ValueError, AddressValueError, NetmaskValueError):
                pass

        if added:
            continue

        # 3. 处理逗号分隔格式（如：103.29.68.0/22,linode）
        if ',' in line:
            parts = line.split(',', 1)
            candidate = parts[0].strip()
            try:
                if '/' in candidate:
                    net = ipaddress.ip_network(candidate, strict=False)
                    if net.prefixlen > 0:
                        ip_list.append(str(net))
                        added = True
                else:
                    addr = ipaddress.ip_address(candidate)
                    ip_list.append(str(addr))
                    added = True
            except (ValueError, AddressValueError, NetmaskValueError):
                pass

        if added:
            continue

        # 4. 处理HTTP/HTTPS URL中的IP
        if line.startswith('http://') or line.startswith('https://'):
            try:
                parsed = urlparse(line)
                host = parsed.netloc
                if host.startswith('[') and ']' in host:
                    ip_end = host.index(']')
                    ip_candidate = host[1:ip_end]
                    try:
                        addr = IPv6Address(ip_candidate)
                        ip_list.append(str(addr))
                        added = True
                    except (ValueError, AddressValueError):
                        pass
                else:
                    host_parts = host.split(':')
                    ip_candidate = host_parts[0]
                    try:
                        addr = IPv4Address(ip_candidate)
                        ip_list.append(str(addr))
                        added = True
                    except (ValueError, AddressValueError):
                        pass
            except Exception:
                pass

        if added:
            continue

        # 5. 使用正则表达式匹配非纯IP行中的IP，但避免误匹配域名中的IP
        ipv4_with_context = re.search(
            r'\b((?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?))\b',
            line
        )
        if ipv4_with_context:
            ip_candidate = ipv4_with_context.group(1)
            try:
                IPv4Address(ip_candidate)
                start_pos = ipv4_with_context.start()
                end_pos = ipv4_with_context.end()
                prev_char = line[start_pos - 1] if start_pos > 0 else ''
                next_char = line[end_pos] if end_pos < len(line) else ''
                if (prev_char and prev_char.isalnum() and not prev_char.isdigit()) or \
                   (next_char and next_char.isalnum() and not next_char.isdigit()):
                    pass
                else:
                    if start_pos == 0 or line[start_pos - 1] in ['/', ':', ' ']:
                        ip_list.append(ip_candidate)
                        added = True
            except (ValueError, AddressValueError):
                pass

        if added:
            continue

        # IPv6匹配
        ipv6_with_context = re.search(ipv6_pattern, line, re.IGNORECASE)
        if ipv6_with_context:
            ip_candidate = ipv6_with_context.group(0)
            try:
                IPv6Address(ip_candidate)
                start_pos = ipv6_with_context.start()
                end_pos = ipv6_with_context.end()
                prev_char = line[start_pos - 1] if start_pos > 0 else ''
                next_char = line[end_pos] if end_pos < len(line) else ''
                if (prev_char and prev_char.isalnum()) or (next_char and next_char.isalnum()):
                    pass
                else:
                    if start_pos == 0 or line[start_pos - 1] in ['/', ':', ' ']:
                        ip_list.append(ip_candidate)
                        added = True
            except (ValueError, AddressValueError):
                pass

        if not added:
            domain_list.append(original_line)

    return ip_list, domain_list

def process_g(temp_dir: Path) -> Tuple[List[str], List[str], List[str]]:
    """处理g组（maltrail仓库）"""
    url = GROUPS['g'][0]
    zip_path = temp_dir / "maltrail.zip"
    
    if not download_file(url, zip_path):
        return [], [], []

    extract_dir = temp_dir / "maltrail_extract"
    if not extract_zip(zip_path, extract_dir):
        return [], [], []

    master_dir = next(extract_dir.glob("maltrail-master"), None)
    if not master_dir:
        logging.warning("Could not find maltrail-master directory")
        return [], [], []

    # 收集所有需要处理的文件
    mixed_lines = []
    files_to_process = [
        master_dir / "misc/worst_asns.txt", 
        master_dir / "trails/custom/dprk.txt"
    ]
    
    static_trails_dir = master_dir / "trails/static"
    if static_trails_dir.is_dir():
        for path in static_trails_dir.rglob("*"):
            if path.is_file() and path.name != "__init__.py":
                files_to_process.append(path)

    # 读取所有文件内容
    for path in files_to_process:
        if path.exists():
            enc = detect_encoding(path)
            try:
                with open(path, 'r', encoding=enc, errors='ignore') as f:
                    mixed_lines.extend(f.readlines())
            except Exception as e:
                logging.warning(f"Failed to read {path}: {e}")

    # 使用专门的maltrail处理函数
    cleaned_mixed = clean_lines(mixed_lines)
    temp9, non_ip_from_maltrail = extract_maltrail_ips_domains(cleaned_mixed)
    temp9 = clean_lines(temp9)

    # 更新customBL.txt
    custom_bl_path = Path("adh/backup/customBL.txt")
    custom_bl_path.parent.mkdir(parents=True, exist_ok=True)
    
    existing_non_ip = []
    if custom_bl_path.exists():
        enc = detect_encoding(custom_bl_path)
        try:
            with open(custom_bl_path, 'r', encoding=enc, errors='ignore') as f:
                existing_non_ip = clean_lines(f.readlines())
        except Exception as e:
            logging.warning(f"Failed to read customBL: {e}")
    
    updated_non_ip = sorted(set(non_ip_from_maltrail + existing_non_ip))
    
    try:
        with open(custom_bl_path, 'w', encoding='utf-8') as f:
            f.write('\n'.join(updated_non_ip) + '\n')
        logging.info(f"Updated custom blocklist with {len(updated_non_ip)} entries")
    except Exception as e:
        logging.error(f"Failed to write customBL: {e}")

    # 处理whitelist.txt
    temp10 = []
    whitelist_path = master_dir / "misc/whitelist.txt"
    if whitelist_path.exists():
        enc = detect_encoding(whitelist_path)
        try:
            with open(whitelist_path, 'r', encoding=enc, errors='ignore') as f:
                lines = f.readlines()
            
            start_index = -1
            for i, line in enumerate(lines):
                if "corp" in line.lower():
                    start_index = i + 1
                    break
            
            if start_index != -1:
                for line in lines[start_index:]:
                    cleaned = enhanced_universal_clean(line)
                    if cleaned and not cleaned.startswith('#'):
                        domain = cleaned.lstrip('www.')
                        if domain:
                            temp10.append(domain)
                
                temp10 = clean_lines(temp10)
                
        except Exception as e:
            logging.warning(f"Failed to process maltrail whitelist: {e}")

    # 处理domain-blocklist.txt中的@@规则
    temp11 = []
    domain_block_path = Path("adh/domain-blocklist.txt")
    if domain_block_path.exists():
        enc = detect_encoding(domain_block_path)
        try:
            with open(domain_block_path, 'r', encoding=enc, errors='ignore') as f:
                for line in f:
                    line_clean = line.strip()
                    if line_clean.startswith('@@'):
                        domain = re.sub(r'^@@(?:\|\||://|\|)?', '', line_clean)
                        domain = re.sub(r'[\|\^/].*$', '', domain)
                        domain = domain.lstrip('www.')
                        if domain:
                            temp11.append(domain)
            
            temp11 = clean_lines(temp11)
            
        except Exception as e:
            logging.warning(f"Failed to process domain blocklist: {e}")

    return temp9, temp10, temp11

def process_h(urls: List[str], temp_dir: Path) -> List[str]:
    """处理h组（白名单）"""
    return process_group(urls, 'h', temp_dir)

def consolidate_networks(ip_list: List[str]) -> List[str]:
    """合并和优化网络列表"""
    networks = set()
    invalid_count = 0
    
    for ip_str in ip_list:
        ip_str = ip_str.split('%')[0]
        
        try:
            if '/' in ip_str:
                net = ipaddress.ip_network(ip_str, strict=False)
            else:
                addr = ipaddress.ip_address(ip_str)
                net = ipaddress.ip_network(f"{addr}/{addr.max_prefixlen}")
            
            if net.prefixlen > 0:
                networks.add(net)
                
        except ValueError:
            invalid_count += 1
    
    if invalid_count > 0:
        logging.info(f"Skipped {invalid_count} invalid IP/CIDR entries")
    
    if not networks:
        return []
    
    ipv4_nets = sorted([n for n in networks if n.version == 4])
    ipv6_nets = sorted([n for n in networks if n.version == 6])
    
    collapsed_v4 = list(ipaddress.collapse_addresses(ipv4_nets))
    collapsed_v6 = list(ipaddress.collapse_addresses(ipv6_nets))
    
    result = []
    for net in collapsed_v4:
        result.append(str(net.network_address) if net.prefixlen == 32 else str(net))
    for net in collapsed_v6:
        result.append(str(net.network_address) if net.prefixlen == 128 else str(net))
    
    return result

def update_whitelist_with_h(temp10: List[str], temp11: List[str], h_whitelist: List[str]) -> None:
    """更新白名单，包含h组内容"""
    whitelist_path = Path("adh/backup/whitelist.txt")
    whitelist_path.parent.mkdir(parents=True, exist_ok=True)
    
    new_items = set(temp10 + temp11 + h_whitelist)
    
    skip_lines = []
    existing_items = set()
    
    if whitelist_path.exists():
        enc = detect_encoding(whitelist_path)
        try:
            with open(whitelist_path, 'r', encoding=enc, errors='ignore') as f:
                lines = f.readlines()
            
            start_idx = -1
            end_idx = -1
            for i, line in enumerate(lines):
                if '# skip start' in line:
                    start_idx = i
                elif '# skip end' in line:
                    end_idx = i
                    break
            
            if start_idx != -1 and end_idx != -1 and start_idx < end_idx:
                skip_lines = lines[start_idx:end_idx + 1]
                other_lines = lines[:start_idx] + lines[end_idx + 1:]
                existing_items = set(clean_lines(other_lines))
            else:
                existing_items = set(clean_lines(lines))
                
        except Exception as e:
            logging.warning(f"Failed to read whitelist: {e}")
    
    all_items = sorted(list(new_items | existing_items))
    
    ipv4_items = []
    ipv6_items = []
    domain_items = []
    
    for item in all_items:
        try:
            ip = ipaddress.ip_address(item)
            if ip.version == 4:
                ipv4_items.append(item)
            else:
                ipv6_items.append(item)
        except ValueError:
            try:
                net = ipaddress.ip_network(item, strict=False)
                if net.prefixlen > 0:
                    if net.version == 4:
                        ipv4_items.append(str(net))
                    else:
                        ipv6_items.append(str(net))
            except ValueError:
                if item:
                    domain_items.append(item)
    
    try:
        with open(whitelist_path, 'w', encoding='utf-8') as f:
            if skip_lines:
                f.writelines(skip_lines)
                f.write('\n')
            
            if ipv4_items:
                f.write('# ipv4\n')
                f.write('\n'.join(sorted(ipv4_items, key=IPv4Address)) + '\n\n')
            
            if ipv6_items:
                f.write('# ipv6\n')
                f.write('\n'.join(sorted(ipv6_items, key=IPv6Address)) + '\n\n')
            
            if domain_items:
                f.write('# domain\n')
                f.write('\n'.join(sorted(domain_items)) + '\n')
                
        logging.info(f"Updated whitelist with {len(ipv4_items)} IPv4, {len(ipv6_items)} IPv6, {len(domain_items)} domains")
        
    except Exception as e:
        logging.error(f"Failed to write whitelist: {e}")

def main() -> None:
    """主函数"""
    try:
        adh_dir = Path('adh')
        backup_dir = adh_dir / 'backup'
        adh_dir.mkdir(parents=True, exist_ok=True)
        backup_dir.mkdir(parents=True, exist_ok=True)
        
        logging.info("Starting IP blocklist processing...")
        
        # Generate ip-blocklist.txt from diff
        a_file = 'adh/blocklist.txt'
        b_file = 'adh/domain-blocklist.txt'
        output_file = 'adh/ip-blocklist.txt'
        if Path(a_file).exists() and Path(b_file).exists():
            ip_count = diff_rules(a_file, b_file, output_file)
            logging.info(f"Generated ip-blocklist.txt with {ip_count} IP rules")
        else:
            logging.warning("Missing blocklist.txt or domain-blocklist.txt, skipping ip-blocklist generation")
        
        with tempfile.TemporaryDirectory() as temp_dir_str:
            temp_dir = Path(temp_dir_str)
            
            regex_map = {
                'f1': r'from\s+([0-9a-fA-F:./]+)',
                'f2': r'--src\s+([0-9a-fA-F:./]+)',
                'f3': r'\{([^}]+)\}',
                'f4': r'access-list\s+ET-all\s+deny\s+ip\s+([0-9.]+)\s+([0-9.]+)\s+any'
            }
            
            with ThreadPoolExecutor(max_workers=10) as executor:
                future_to_group = {
                    executor.submit(
                        process_group, 
                        GROUPS['a'] + GROUPS['b1'] + GROUPS['b2'] + GROUPS['b3'], 
                        'a_b', 
                        temp_dir
                    ): 'a_b',
                    executor.submit(process_c, GROUPS['c'], temp_dir): 'c',
                    executor.submit(
                        lambda: clean_lines(
                            process_d('d1', GROUPS['d1'], True, temp_dir) + 
                            process_d('d2', GROUPS['d2'], False, temp_dir)
                        )
                    ): 'd',
                    executor.submit(process_e, GROUPS['e'], temp_dir): 'e',
                    executor.submit(process_f, 'f1', GROUPS['f1'], regex_map['f1'], temp_dir): 'f1',
                    executor.submit(process_f, 'f2', GROUPS['f2'], regex_map['f2'], temp_dir): 'f2',
                    executor.submit(process_f, 'f3', GROUPS['f3'], regex_map['f3'], temp_dir): 'f3',
                    executor.submit(process_f, 'f4', GROUPS['f4'], regex_map['f4'], temp_dir): 'f4',
                    executor.submit(process_g, temp_dir): 'g',
                    executor.submit(process_h, GROUPS['h'], temp_dir): 'h'
                }
                
                results = {}
                for future in as_completed(future_to_group):
                    group_name = future_to_group[future]
                    try:
                        result = future.result()
                        if group_name == 'g':
                            results[group_name] = result
                            logging.info(f"Group {group_name} processed: {len(result[0])} IPs, {len(result[1])} domains, {len(result[2])} rules")
                        else:
                            results[group_name] = result
                            logging.info(f"Group {group_name} processed: {len(result)} entries")
                    except Exception as e:
                        logging.error(f"Error processing group {group_name}: {e}")
                        results[group_name] = [] if group_name != 'g' else ([], [], [])
            
            temp1 = results.get('a_b', [])
            temp2 = results.get('c', [])
            temp3 = results.get('d', [])
            temp4 = results.get('e', [])
            temp5 = results.get('f1', [])
            temp6 = results.get('f2', [])
            temp7 = results.get('f3', [])
            temp8 = results.get('f4', [])
            temp9, temp10, temp11 = results.get('g', ([], [], []))
            h_whitelist = results.get('h', [])
            
            logging.info(
                f"Processing complete. Found:\n"
                f"- Group a/b: {len(temp1)} entries\n"
                f"- Group c: {len(temp2)} entries\n" 
                f"- Group d: {len(temp3)} entries\n"
                f"- Group e: {len(temp4)} entries\n"
                f"- Group f: {len(temp5) + len(temp6) + len(temp7) + len(temp8)} entries\n"
                f"- Group g: {len(temp9)} IPs, {len(temp10)} domains, {len(temp11)} rules\n"
                f"- Group h: {len(h_whitelist)} whitelist entries"
            )
            
            all_temp_ips = (
                temp1 + temp2 + temp3 + temp4 + temp5 + 
                temp6 + temp7 + temp8 + temp9
            )
            
            ip_blocklist_path = Path(output_file)
            generated_blocklist = []
            if ip_blocklist_path.exists():
                enc = detect_encoding(ip_blocklist_path)
                try:
                    with open(ip_blocklist_path, 'r', encoding=enc, errors='ignore') as f:
                        generated_blocklist = clean_lines(f.readlines())
                    logging.info(f"Read generated ip-blocklist: {len(generated_blocklist)} entries")
                except Exception as e:
                    logging.warning(f"Failed to read generated ip-blocklist: {e}")
            
            all_ips = all_temp_ips + generated_blocklist
            
            consolidated_temp = consolidate_networks(all_ips)
            logging.info(f"Consolidated IPs: {len(consolidated_temp)} entries")
            
            backup_ip_path = backup_dir / "ip.txt"
            existing_backup = []
            if backup_ip_path.exists():
                enc = detect_encoding(backup_ip_path)
                try:
                    with open(backup_ip_path, 'r', encoding=enc, errors='ignore') as f:
                        existing_backup = clean_lines(f.readlines())
                    logging.info(f"Read existing backup: {len(existing_backup)} entries")
                except Exception as e:
                    logging.warning(f"Failed to read backup ip: {e}")
            
            final_ip_list = consolidate_networks(consolidated_temp + existing_backup)
            
            try:
                with open(backup_ip_path, 'w', encoding='utf-8') as f:
                    f.write('\n'.join(final_ip_list) + '\n')
                logging.info(f"Updated main IP blocklist with {len(final_ip_list)} entries")
            except Exception as e:
                logging.error(f"Failed to write IP blocklist: {e}")
            
            update_whitelist_with_h(temp10, temp11, h_whitelist)
        
        logging.info("All tasks completed successfully")
    
    except Exception as e:
        logging.error(f"Script execution failed: {e}")
        raise

if __name__ == "__main__":
    main()
